{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "45d26be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11.9M/11.9M [00:00<00:00, 18.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size:  11314\n",
      "test_size:  7532\n",
      "vocab_size:  5000\n",
      "average length: 110.543\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import topmost  \n",
    "torch.cuda.is_available()\n",
    "\n",
    "import topmost\n",
    "from topmost.data import download_dataset\n",
    "\n",
    "device = \"cuda\" # or \"cpu\"\n",
    "dataset_dir = \"./datasets/20NG\"\n",
    "download_dataset('20NG', cache_path='./datasets')\n",
    "\n",
    "dataset = topmost.BasicDataset(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b06cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import topmost.utils\n",
    "import topmost.utils._utils\n",
    "from topmost import eva\n",
    "\n",
    "\n",
    "class Beta_env():\n",
    "    def __init__(self, dataset, max_steps, num_topics, embed_size, device, reference_corpus, num_top_words = 5, random = False):\n",
    "        self.device = device\n",
    "        self.num_topics = num_topics\n",
    "        self.embed_size = embed_size\n",
    "        self.random_init = random\n",
    "        self.step_counter = 0\n",
    "        self.max_steps = max_steps\n",
    "        self.num_top_words = num_top_words\n",
    "        self.reference_corpus = reference_corpus\n",
    "        self.dataset = dataset\n",
    "        self.vocab = dataset.vocab\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "        self.str_to_embeds = {\n",
    "            dataset.vocab[i]:torch.tensor(\n",
    "                dataset.pretrained_WE[i], device = device\n",
    "            ) for i in range(0, len(dataset.vocab))\n",
    "        }\n",
    "        \n",
    "        self.beta, self.topic_embeds = self.get_starting_state()\n",
    "\n",
    "\n",
    "    def get_starting_state(self):\n",
    "        if self.random_init:\n",
    "            beta = torch.rand(size=(self.num_topics, self.vocab_size), device = self.device)\n",
    "            topic_embeds = torch.rand(size = (self.num_topics, self.embed_size), device = self.device)\n",
    "        else:\n",
    "            beta, topic_embeds = None, None\n",
    "        return beta, topic_embeds\n",
    "\n",
    "    def reset(self):\n",
    "        self.step_counter = 0\n",
    "        self.beta, self.topic_embeds = self.get_starting_state()\n",
    "        return torch.concatenate((self.beta, self.topic_embeds), dim = 1)\n",
    "    \n",
    "    \n",
    "    def compute_cosine_similarity(self, list_txt):\n",
    "        result = torch.zeros((1,1), device = self.device)\n",
    "        for i in range(self.num_topics):\n",
    "            for word in list_txt[i].split():\n",
    "                we = self.str_to_embeds[word]\n",
    "                result += torch.cosine_similarity(we, self.topic_embeds[i, :], dim = 0)\n",
    "        return result.cpu().numpy().sum()\n",
    "        \n",
    "    def calculate_reward(self):\n",
    "        top_words = self.get_top_words(False)\n",
    "        total_TD =  eva.topic_diversity._diversity(top_words)\n",
    "\n",
    "        total_cosine_similarity = self.compute_cosine_similarity(top_words)\n",
    "        \n",
    "        return  (total_TD / self.num_topics + total_cosine_similarity / (self.num_topics * self.num_top_words)) / 2\n",
    "        \n",
    "\n",
    "    def get_top_words(self, verbose = True):\n",
    "        top_word_list = topmost.utils._utils.get_top_words(self.beta.cpu().numpy(), self.vocab, num_top_words=self.num_top_words, verbose=verbose)\n",
    "        return top_word_list\n",
    "\n",
    "    def step(self, action):\n",
    "        self.beta += action\n",
    "        new_state = torch.concatenate((self.beta, self.topic_embeds), dim = 1)\n",
    "        reward = self.calculate_reward()\n",
    "        self.step_counter += 1\n",
    "        done = False\n",
    "        if self.step_counter == self.max_steps:\n",
    "            done = True\n",
    "            self.step_counter = 0\n",
    "        \n",
    "        self.state = new_state\n",
    "        return self.state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "17136762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11632000327110291"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Beta_env(\n",
    "    dataset = dataset,\n",
    "    max_steps=3,\n",
    "    num_topics=5,\n",
    "    embed_size=200,\n",
    "    device = \"cuda\",\n",
    "    reference_corpus=dataset.train_texts,\n",
    "    random = True\n",
    ")\n",
    "# x = env.get_top_words()\n",
    "env.calculate_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "384cff58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 4.7914,  3.7741, -2.6307,  ...,  0.7557,  0.8145,  0.8498],\n",
       "         [-3.6298, -1.4636,  2.3268,  ...,  0.9754,  0.9951,  0.3757],\n",
       "         [-1.7730,  4.7649,  2.6474,  ...,  0.8158,  0.5052,  0.5368],\n",
       "         [ 0.4783,  0.1749, -1.8445,  ...,  0.9176,  0.6082,  0.7910],\n",
       "         [-0.1448,  1.8021,  0.7093,  ...,  0.0653,  0.1500,  0.3311]],\n",
       "        device='cuda:0'),\n",
       " 0.11993046402931214,\n",
       " False)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = torch.randn(size = (5, 5000), device = device)\n",
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf7437c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
