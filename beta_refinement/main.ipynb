{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "7361202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import topmost.utils\n",
    "import topmost.utils._utils\n",
    "from topmost import eva\n",
    "import numpy as np\n",
    "\n",
    "class Beta_env():\n",
    "    def __init__(self, dataset, max_steps, num_topics, embed_size, reference_corpus, num_top_words = 5, random = False):\n",
    "\n",
    "        self.num_topics = num_topics\n",
    "        self.embed_size = embed_size\n",
    "        self.random_init = random\n",
    "        self.step_counter = 0\n",
    "        self.max_steps = max_steps\n",
    "        self.num_top_words = num_top_words\n",
    "        self.reference_corpus = reference_corpus\n",
    "        self.dataset = dataset\n",
    "        self.vocab = dataset.vocab\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        # self.vocab_size = 100\n",
    "\n",
    "        self.str_to_embeds = {\n",
    "            dataset.vocab[i]:dataset.pretrained_WE[i]\n",
    "            for i in range(0, self.vocab_size)\n",
    "        }\n",
    "        \n",
    "        self.beta, self.topic_embeds = self.get_starting_state()\n",
    "\n",
    "\n",
    "    def get_starting_state(self):\n",
    "        if self.random_init:\n",
    "            beta = np.random.normal(loc = 0, scale=1, size = (self.num_topics, self.vocab_size))\n",
    "            topic_embeds = np.random.normal(loc = 0, scale = 1, size = (self.num_topics, self.embed_size))\n",
    "        else:\n",
    "            beta, topic_embeds = None, None\n",
    "        return beta, topic_embeds\n",
    "\n",
    "    def reset(self):\n",
    "        self.step_counter = 0\n",
    "        self.beta, self.topic_embeds = self.get_starting_state()\n",
    "        return np.concatenate((self.beta, self.topic_embeds), axis = 1)\n",
    "    \n",
    "    def cosine_similarity(self, a, b):\n",
    "        a = np.asarray(a)\n",
    "        b = np.asarray(b)\n",
    "        if np.linalg.norm(a) == 0 or np.linalg.norm(b) == 0:\n",
    "            return np.asarray([0])\n",
    "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    \n",
    "    def compute_cosine_similarity(self, list_txt):\n",
    "        result = 0\n",
    "        for i in range(self.num_topics):\n",
    "            for word in list_txt[i].split():\n",
    "                we = self.str_to_embeds[word]\n",
    "                result += self.cosine_similarity(we, self.topic_embeds[i, :]).item()\n",
    "        return result\n",
    "        \n",
    "    def calculate_reward(self):\n",
    "        top_words = self.get_top_words(False)\n",
    "        total_TD =  eva.topic_diversity._diversity(top_words)\n",
    "\n",
    "        total_cosine_similarity = self.compute_cosine_similarity(top_words)\n",
    "        return (total_TD + total_cosine_similarity / (self.num_topics * self.num_top_words)) / 2\n",
    "        \n",
    "\n",
    "    def get_top_words(self, verbose = True):\n",
    "        top_word_list = topmost.utils._utils.get_top_words(self.beta, self.vocab, num_top_words=self.num_top_words, verbose=verbose)\n",
    "        return top_word_list\n",
    "\n",
    "    def step(self, action):\n",
    "        self.beta += action\n",
    "        new_state = np.concatenate((self.beta, self.topic_embeds), axis = 1)\n",
    "        reward = self.calculate_reward()\n",
    "        self.step_counter += 1\n",
    "        done = False\n",
    "        if self.step_counter == self.max_steps:\n",
    "            done = True\n",
    "            self.step_counter = 0\n",
    "        \n",
    "        self.state = new_state\n",
    "        return self.state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "7a20c829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11.9M/11.9M [00:00<00:00, 18.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size:  11314\n",
      "test_size:  7532\n",
      "vocab_size:  5000\n",
      "average length: 110.543\n"
     ]
    }
   ],
   "source": [
    "from topmost.data import download_dataset\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dataset_dir = \"./datasets/20NG\"\n",
    "download_dataset('20NG', cache_path='./datasets')\n",
    "\n",
    "dataset = topmost.BasicDataset(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "8c73a1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.497744489060825"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Beta_env(\n",
    "    dataset = dataset,\n",
    "    max_steps=3,\n",
    "    num_topics=5,\n",
    "    embed_size=200,\n",
    "    reference_corpus=dataset.train_texts,\n",
    "    random = True\n",
    ")\n",
    "env.calculate_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "c40225f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: tiff tcp returned either releases\n",
      "Topic 1: collapse particularly coast stops activities\n",
      "Topic 2: fathers christopher members cutting possibilities\n",
      "Topic 3: friends owns terrorist mormons funding\n",
      "Topic 4: people examined unlike line husband\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tiff tcp returned either releases',\n",
       " 'collapse particularly coast stops activities',\n",
       " 'fathers christopher members cutting possibilities',\n",
       " 'friends owns terrorist mormons funding',\n",
       " 'people examined unlike line husband']"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_top_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "8ab461ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 2.14733779, -0.05049328,  1.67646017, ..., -1.28830018,\n",
       "          0.40203184, -0.12363293],\n",
       "        [-2.81581479, -0.16239032,  1.78195767, ..., -2.09482724,\n",
       "          0.76477559,  0.58207478],\n",
       "        [-3.19513548, -0.0234362 ,  0.16951657, ...,  1.69616629,\n",
       "         -1.16462347,  1.39557918],\n",
       "        [ 2.21971839, -0.08290676,  1.97690823, ..., -0.57450664,\n",
       "          0.93220443,  1.50757706],\n",
       "        [-0.54415543, -0.35779767, -1.82209127, ..., -0.72354627,\n",
       "          0.32849414, -1.44888498]]),\n",
       " 0.5123433505875354,\n",
       " False)"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = np.random.normal(loc = 0, scale = 1, size = env.beta.shape)\n",
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "1067f176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5200)"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset().shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
